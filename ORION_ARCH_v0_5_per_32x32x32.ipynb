{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SI8Y_ZxQ5158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a4ca9f-ee10-4313-d556-c67ef25f0b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "data_dir = \"/content/drive/My Drive/Dataset_download/\"\n",
        "save_path = \"/content/drive/My Drive/pth_results-RUN2/\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    # Create the folder\n",
        "    os.makedirs(save_path)\n",
        "    print(f\"Folder '{save_path}' created successfully.\")\n",
        "\n",
        "\n",
        "hdf5_file_path = \"/content/drive/My Drive/dataset_40_32.h5\"\n",
        "ORIENT_LABELS = 18\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VoxelDatasetHDF5(Dataset):\n",
        "    def __init__(self, hdf5_file, train=True, transform=None):\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "\n",
        "        # Open the HDF5 file\n",
        "        with h5py.File(self.hdf5_file, 'r') as f:\n",
        "            # Get class names from the structure of the HDF5 file\n",
        "            self.classes = sorted(list(f['classes'].keys()))  # Assuming top-level is 'classes'\n",
        "\n",
        "            # Collect all paths (keys) to the datasets inside HDF5\n",
        "            self.file_paths = []\n",
        "            for class_name in self.classes:\n",
        "                folder = 'train' if train else 'test'\n",
        "                class_group = f['classes'][class_name][folder]\n",
        "                self.file_paths += [(class_name, folder, file_name) for file_name in class_group.keys()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        class_name, folder, file_name = self.file_paths[idx]\n",
        "\n",
        "        # Detect corrupted files based on the presence of \"(1)\" in the file name\n",
        "        if \"(1)\" in file_name:\n",
        "            print(f\"Corrupted file detected and skipped: {file_name}\")\n",
        "            return None  # Skip this file\n",
        "\n",
        "        # Load voxelized numpy array from the HDF5 file\n",
        "        with h5py.File(self.hdf5_file, 'r') as f:\n",
        "            voxel_data = f['classes'][class_name][folder][file_name][()]\n",
        "\n",
        "        # Add a channel dimension: Shape (1, 33, 33, 33)\n",
        "        voxel_data = np.expand_dims(voxel_data, axis=0)\n",
        "\n",
        "        if self.transform:\n",
        "            voxel_data = self.transform(voxel_data)\n",
        "\n",
        "        voxel_data = torch.tensor(voxel_data, dtype=torch.float32)\n",
        "\n",
        "        # Get class label\n",
        "        class_label = self.classes.index(class_name)\n",
        "\n",
        "        # Extract the orientation from the file name and convert to a bin index based on the number of bins for this class\n",
        "        try:\n",
        "            orientation_number = int(file_name.split('_')[-1].split('.')[0])\n",
        "        except:\n",
        "            orientation_number = 17  # Handle error case\n",
        "            print(f\"Error parsing orientation for file: {file_name}\")\n",
        "\n",
        "        orientation_label = torch.tensor(orientation_number, dtype=torch.long)\n",
        "\n",
        "        return voxel_data, class_label, orientation_label"
      ],
      "metadata": {
        "id": "oubjLPv4SPdj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedORION(nn.Module):\n",
        "    def __init__(self, num_class=40, num_orient=105):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_net = nn.Sequential(\n",
        "            #conv1\n",
        "            nn.Conv3d(1,32,3,stride=2,padding=1), #1x 32^3 -> 32x 16^3\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.LeakyReLU(0.1,True),\n",
        "            nn.Dropout3d(0.2),\n",
        "\n",
        "            #conv2\n",
        "            nn.Conv3d(32,64,3,1,1), #32x 16^3 -> 64x 16^3\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.LeakyReLU(0.1,True),\n",
        "            nn.Dropout3d(0.3),\n",
        "\n",
        "            #conv3\n",
        "            nn.Conv3d(64,128,3,1,0), #64x 14^3 -> 128x 12^3\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.LeakyReLU(0.1,True),\n",
        "            #pool3\n",
        "            nn.MaxPool3d(2,2), #128x 12^3 -> 128x 6^3\n",
        "            nn.Dropout3d(0.4),\n",
        "\n",
        "            #conv4\n",
        "            nn.Conv3d(128,256,3,1,0), #128x 6^3 -> 256x 4^3\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.LeakyReLU(0.1,True),\n",
        "            #pool4\n",
        "            nn.MaxPool3d(2,2), #256x 4^3 -> 256x 2^3\n",
        "            nn.Dropout3d(0.6),\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten() #256x 2^3 -> 2048\n",
        "\n",
        "        self.fc_net = nn.Sequential(\n",
        "            nn.Linear(256*2**3, 256), #2048 -> 256\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Linear(256, num_class + num_orient), #256 -> \"40+105\" (for simplicity)\n",
        "        )\n",
        "        # Initialize the weights\n",
        "        self.num_class = num_class\n",
        "        self.num_orient = num_orient\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_net(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc_net(x)\n",
        "\n",
        "        # Split the output into class and orientation\n",
        "        class_output = x[:, :self.num_class]  # First part for class\n",
        "        orient_output = x[:, self.num_class:]  # Remaining part for orientation\n",
        "        return class_output, orient_output\n",
        "\n",
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, neurons_per_class):\n",
        "        super(CustomCrossEntropyLoss, self).__init__()\n",
        "        self.neurons_per_class = neurons_per_class\n",
        "\n",
        "        self.incremental = np.cumsum(self.neurons_per_class)\n",
        "        self.incremental = np.insert(self.incremental, 0, 0)\n",
        "        self.incremental = self.incremental[:-1]\n",
        "        #se i neuroni per classe sono:  [6 3 6 6  6  9  3  6  1  9]\n",
        "        #gli starting points sono:      [0 6 9 15 21 27 36 39 45 46]\n",
        "\n",
        "        self.scaling_factor = np.array([ ORIENT_LABELS // self.neurons_per_class[i] for i in range(len(self.neurons_per_class))])\n",
        "        #di quanto devo dividere i label per ottenere il neurone corretto\n",
        "        #se i neuroni per le classi 0,1 sono: [6     3]\n",
        "        #lo scaling factor è:                 [18/6  18/3]\n",
        "\n",
        "        #print(\"\\nNEURONI PER CLASSE:\", self.neurons_per_class)\n",
        "        #print(\"INCREMENTALI:\", self.incremental,\"\\n\")\n",
        "\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    #                     10/40,          105,         1-10/40         1-18\n",
        "    def forward(self, class_neurons, orient_neurons, class_labels, orient_labels):\n",
        "\n",
        "        #print(\"labels = \", orient_labels)\n",
        "        #remap the labels\n",
        "        remapped_labels = torch.zeros_like(orient_labels)\n",
        "        for i in range(orient_labels.shape[0]):\n",
        "            remapped_labels[i] = self.incremental[class_labels[i]] + orient_labels[i] // self.scaling_factor[class_labels[i]]\n",
        "            #sposto il label sullo starting point per la classe  + offset relativo in base al numero di neuroni\n",
        "        #print(\"remapped labels = \", orient_labels)\n",
        "\n",
        "        class_loss = self.cross_entropy(class_neurons, class_labels)\n",
        "        orient_loss = self.cross_entropy(orient_neurons, remapped_labels)\n",
        "\n",
        "        #print(\"class_loss = \", class_loss)\n",
        "        #print(\"orient_loss = \", orient_loss)\n",
        "\n",
        "        return class_loss, orient_loss"
      ],
      "metadata": {
        "id": "MWzET4HuMnBN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, start_epoch=0, num_epochs=10, device=device, gamma=0.5, skip_training_go_valid=False):\n",
        "    model.to(device)\n",
        "\n",
        "    #dont train the loss\n",
        "    for param in criterion.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "        #training step\n",
        "        print(\"TRAINING Epoch #\",epoch)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Loop over the dataset\n",
        "        if not skip_training_go_valid:\n",
        "            for voxel_data, class_labels, orientation_labels in tqdm(train_loader):\n",
        "                # Move data to the device\n",
        "                voxel_data = voxel_data.to(device)\n",
        "                class_labels = class_labels.to(device)\n",
        "                orientation_labels = orientation_labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                class_outputs, orient_outputs = model(voxel_data)\n",
        "\n",
        "                # Compute loss\n",
        "                class_loss, orient_loss = criterion(class_outputs, orient_outputs, class_labels, orientation_labels)\n",
        "                loss = gamma * class_loss + (1-gamma) * orient_loss\n",
        "\n",
        "                # Backpropagation and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                #print(f'Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "            # Print statistics at the end of every training epoch\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "            save_model(epoch, model, optimizer, loss.item(), f\"{save_path}model_ORION_0.5_per32x32x32_epoch_{epoch}_batchsize_{batch_size}_numw{num_workers}.pth\")\n",
        "\n",
        "        #validation step\n",
        "        print(\"VALIDATION Epoch #\",epoch)\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        correct_class = 0\n",
        "        correct_orient = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for voxel_data, class_labels, orientation_labels in tqdm(test_loader):\n",
        "                voxel_data = voxel_data.to(device)\n",
        "                class_labels = class_labels.to(device)\n",
        "                orientation_labels = orientation_labels.to(device)\n",
        "\n",
        "                class_outputs, orient_outputs = model(voxel_data)\n",
        "\n",
        "                class_loss, orient_loss = criterion(class_outputs, orient_outputs, class_labels, orientation_labels)\n",
        "                total_loss = gamma * class_loss + (1-gamma) * orient_loss\n",
        "\n",
        "                valid_loss += total_loss.item()\n",
        "\n",
        "                #exctract the one with highest prob\n",
        "                _, predicted_class = torch.max(class_outputs, 1)\n",
        "\n",
        "                total_samples += class_labels.size(0)\n",
        "                correct_class += (class_labels == predicted_class).sum().item()\n",
        "\n",
        "                #remap the labels before performing the accuracy metric with orientations\n",
        "                remapped_labels = torch.zeros_like(orientation_labels)\n",
        "                predicted_orient = [[] for _ in range(orientation_labels.shape[0])]\n",
        "                for i in range(orientation_labels.shape[0]):\n",
        "                    remapped_labels[i] = criterion.incremental[class_labels[i]] + orientation_labels[i] // criterion.scaling_factor[class_labels[i]]\n",
        "                    metric = max(1, criterion.neurons_per_class[class_labels[i]]//3)\n",
        "                    _, predicted_orient[i] = torch.topk(orient_outputs[i], metric) #top-1/2/3\n",
        "\n",
        "                    if remapped_labels[i] in predicted_orient[i]:\n",
        "                        correct_orient += 1\n",
        "\n",
        "                #print(f'correct_class: {correct_class}/{total_samples} correct_orient: {correct_orient}/{total_samples}')\n",
        "\n",
        "        class_accuracy = correct_class / total_samples\n",
        "        orient_accuracy = correct_orient / total_samples\n",
        "\n",
        "        print(f'Class Accuracy: {class_accuracy:.4f}')\n",
        "        print(f'Orientation Accuracy: {orient_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "Cx6476SOSVZu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(epoch, model, optimizer, loss, save_path=save_path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Load model and optimizer state dicts\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch'] +1\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    print(f\"Checkpoint loaded model from epoch {epoch-1} with loss {loss}. Resuming next with epoch {epoch}\")\n",
        "\n",
        "    return model, optimizer, epoch, loss"
      ],
      "metadata": {
        "id": "mOlDS9YhSXD7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2\n",
        "batch_size = 64\n",
        "train_dataset = VoxelDatasetHDF5(hdf5_file=hdf5_file_path, train=True)\n",
        "test_dataset = VoxelDatasetHDF5(hdf5_file=hdf5_file_path, train=False)"
      ],
      "metadata": {
        "id": "LbfjMpsm8q_m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "Ffs2r50WaF13"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ModelNet10\n",
        "#neurons_per_class = np.array([6, 3, 6, 6, 6, 9, 3, 6, 1, 9])\n",
        "#ModelNet40\n",
        "neurons_per_class = np.array([6, 6, 1, 6, 6, 1, 1, 3, 6, 1, 3, 9, 6, 6, 6, 3, 3, 9, 1, 3, 6, 6, 9, 3, 6, 9, 6, 3, 6, 6, 6, 9, 3, 1, 3, 6, 3, 1, 6, 6])\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "num_class = len(neurons_per_class)\n",
        "num_orient = neurons_per_class.sum()  # Customize based on your setup\n",
        "print(\"num_class = \",num_class)\n",
        "print(\"num_orient = \",num_orient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Cb0GsEZtgM",
        "outputId": "4cc2ce60-96c0-4689-c089-f6db801f2e11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_class =  40\n",
            "num_orient =  190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ImprovedORION(num_class=num_class, num_orient=num_orient)\n",
        "criterion = CustomCrossEntropyLoss(neurons_per_class=neurons_per_class)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming 'model' and 'optimizer' are already defined\n",
        "start_epoch = 0\n",
        "model, optimizer, start_epoch, last_loss = load_checkpoint(model, optimizer, f\"{save_path}model_ORION_0.5_per32x32x32_epoch_{0}_batchsize_{batch_size}_numw{num_workers}.pth\", device=device)\n",
        "\n",
        "# Make sure to move optimizer's internal state to the correct device\n",
        "if(start_epoch > 0):\n",
        "    for state in optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)\n",
        "\n",
        "#resume - use \"skip_train_go_valid=True\" if pytorch merd blocked after training and you want to do just the valid step\n",
        "train_model(model, train_loader, criterion, optimizer, start_epoch, num_epochs=10, device=device, gamma=0.5)"
      ],
      "metadata": {
        "id": "VNFvnn2N8tB6",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44015cb1-79be-4e16-f15e-2996add4e76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c76edbf797f0>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded model from epoch 0 with loss 1.2415742874145508. Resuming next with epoch 1\n",
            "TRAINING Epoch # 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 880/2645 [05:07<10:25,  2.82it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UN PO' DI PLOTS CHE A MORSELS PIACCIONO ##"
      ],
      "metadata": {
        "id": "hj4KKzlosMPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32 x 32 x 32 ###"
      ],
      "metadata": {
        "id": "Q4bVL2gfLS30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your checkpoint files are named 'checkpoint_epoch_X.pth' where X is the epoch number\n",
        "num_epochs = 9  # Or however many epochs you have\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    checkpoint_path = save_path+f\"model_ORION_0.5_per32x32x32_epoch_{epoch}_batchsize_64_numw0.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')  # Load checkpoint\n",
        "    losses.append(checkpoint['loss'])  # Extract and store the loss\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(range(1, num_epochs + 1), losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cGCdKqiWllOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33 x 33 x 33 ###"
      ],
      "metadata": {
        "id": "Hrkc-_9LLZA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your checkpoint files are named 'checkpoint_epoch_X.pth' where X is the epoch number\n",
        "num_epochs = 9  # Or however many epochs you have\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    checkpoint_path = save_path+f\"model_epoch_{epoch}.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')  # Load checkpoint\n",
        "    losses.append(checkpoint['loss'])  # Extract and store the loss\n",
        "\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(range(1, num_epochs + 1), losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wKK22xHZLiEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming checkpoints are named 'checkpoint_epoch_X.pth'\n",
        "num_epochs = 6  # Adjust this based on the number of checkpoints\n",
        "gamma = 0.5  # Replace with your actual gamma value\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Lists to store the losses and accuracies for each epoch\n",
        "validation_losses = []\n",
        "class_accuracies = []\n",
        "orient_accuracies = []\n",
        "\n",
        "# Validation step for each checkpoint\n",
        "for epoch in range(num_epochs):\n",
        "    checkpoint_path = save_path+f\"model_ORION_0.5_per32x32x32_epoch_{epoch}_batchsize_64_numw0.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Load model and optimizer states\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Perform validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_class = 0\n",
        "    correct_orient = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for voxel_data, class_labels, orientation_labels in tqdm(test_loader):\n",
        "            voxel_data = voxel_data.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "            orientation_labels = orientation_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            class_outputs, orient_outputs = model(voxel_data)\n",
        "\n",
        "            # Compute loss\n",
        "            class_loss, orient_loss = criterion(class_outputs, orient_outputs, class_labels, orientation_labels)\n",
        "            total_loss = gamma * class_loss + (1 - gamma) * orient_loss\n",
        "            valid_loss += total_loss.item()\n",
        "\n",
        "            # Accuracy for classes\n",
        "            _, predicted_class = torch.max(class_outputs, 1)\n",
        "            total_samples += class_labels.size(0)\n",
        "            correct_class += (class_labels == predicted_class).sum().item()\n",
        "\n",
        "            # Orientation accuracy\n",
        "            remapped_labels = torch.zeros_like(orientation_labels)\n",
        "            predicted_orient = [[] for _ in range(orientation_labels.shape[0])]\n",
        "            for i in range(orientation_labels.shape[0]):\n",
        "                remapped_labels[i] = criterion.incremental[class_labels[i]] + orientation_labels[i] // criterion.scaling_factor[class_labels[i]]\n",
        "                metric = max(1, criterion.neurons_per_class[class_labels[i]] // 3)\n",
        "                _, predicted_orient[i] = torch.topk(orient_outputs[i], metric)\n",
        "\n",
        "                if remapped_labels[i] in predicted_orient[i]:\n",
        "                    correct_orient += 1\n",
        "\n",
        "    # Calculate average loss and accuracy for this epoch\n",
        "    avg_valid_loss = valid_loss / len(test_loader)\n",
        "    class_accuracy = correct_class / total_samples\n",
        "    orient_accuracy = correct_orient / total_samples\n",
        "\n",
        "    # Store results for plotting\n",
        "    validation_losses.append(avg_valid_loss)\n",
        "    class_accuracies.append(class_accuracy)\n",
        "    orient_accuracies.append(orient_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_valid_loss:.4f}, \"\n",
        "          f\"Class Accuracy: {class_accuracy:.4f}, Orientation Accuracy: {orient_accuracy:.4f}\")\n",
        "\n",
        "# Plot validation loss\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs + 1), validation_losses, marker='o', label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot class accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs + 1), class_accuracies, marker='o', label=\"Class Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Class Accuracy Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot orientation accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs + 1), orient_accuracies, marker='o', label=\"Orientation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Orientation Accuracy Over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VRNINUh9o8Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def recompute_training_loss(checkpoint_path, model, train_loader, criterion, device):\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights\n",
        "    model.to(device)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for voxel_data, class_labels, orientation_labels in tqdm(train_loader):\n",
        "            # Move data to device in one step\n",
        "            voxel_data, class_labels, orientation_labels = voxel_data.to(device), class_labels.to(device), orientation_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            class_outputs, orient_outputs = model(voxel_data)\n",
        "            class_loss, orient_loss = criterion(class_outputs, orient_outputs, class_labels, orientation_labels)\n",
        "            total_loss += (class_loss + orient_loss).item() * voxel_data.size(0)\n",
        "            total_samples += voxel_data.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    return avg_loss\n",
        "\n",
        "# Example usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Adjusted batch size and number of workers\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=256,  # Increase batch size\n",
        "    shuffle=False,\n",
        "    num_workers=2  # Adjust this based on your hardware capacity\n",
        ")\n",
        "\n",
        "train_losses = []\n",
        "num_epochs = 6  # Adjust based on the number of checkpoints\n",
        "\n",
        "# Loop through each checkpoint\n",
        "for epoch in range(num_epochs):\n",
        "    checkpoint_path = save_path + f\"model_ORION_0.5_per32x32x32_epoch_{epoch}_batchsize_64_numw0.pth\"\n",
        "    avg_loss = recompute_training_loss(checkpoint_path, model, train_loader, criterion, device)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Recomputed training loss for epoch {epoch + 1}: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot the recomputed training loss over epochs\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Recomputed Training Loss Over Epochs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5BfelIUzreAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DA QUI IN POI CODICE DI PROVA\n"
      ],
      "metadata": {
        "id": "fUYva-xqjPtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#da un idea della top-1/2/3 flex\n",
        "print(criterion.neurons_per_class)\n",
        "for i in range(40):\n",
        "  _, ind = torch.topk(torch.tensor([0,1,2,3,4,5,6,7,8]), max(1, criterion.neurons_per_class[i]//3))\n",
        "  print(ind)"
      ],
      "metadata": {
        "id": "J9wFOi_9_1jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stampa gli output dei neuroni\n",
        "model.to(device)\n",
        "model.eval()\n",
        "num_samples_to_infer = 10\n",
        "sample_data = []\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
        "for i, (voxel_data, class_labels, orientation_labels) in enumerate(test_loader):\n",
        "  if i < num_samples_to_infer:\n",
        "    sample_data.append((voxel_data, class_labels, orientation_labels))\n",
        "  else:\n",
        "    break\n",
        "\n",
        "# Perform inference\n",
        "for voxel_data, class_labels, orientation_labels in sample_data:\n",
        "  with torch.no_grad():\n",
        "    voxel_data = voxel_data.to(device)\n",
        "    class_outputs, orient_outputs = model(voxel_data)\n",
        "\n",
        "    _, predicted_class = torch.max(class_outputs, 1)\n",
        "    predicted_class_values = predicted_class.cpu().numpy()\n",
        "    class_outputs_np = class_outputs.cpu().detach().numpy()\n",
        "\n",
        "    _, predicted_orient = torch.max(orient_outputs, 1)\n",
        "    predicted_orient_values = predicted_orient.cpu().numpy()\n",
        "    orient_outputs_np = orient_outputs.cpu().detach().numpy()\n",
        "\n",
        "    orientation_labels = criterion.incremental[class_labels] + orientation_labels // criterion.scaling_factor[class_labels]\n",
        "\n",
        "    print(orientation_labels)\n",
        "    print(predicted_orient_values)\n",
        "    plt.figure(figsize=(10, 5))  # Adjust figure size as needed\n",
        "    plt.bar(np.arange(190), orient_outputs_np[0])  # Plot for the first sample in the batch\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Output Value\")\n",
        "    plt.title(\"Class Outputs\")\n",
        "    plt.xticks(np.arange(40))  # Set x-axis ticks\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "m9us9p2Kjlj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stampa i nomi dei layer del modello\n",
        "print(model)\n",
        "for name, layer in model.named_modules():\n",
        "  print(layer,name)"
      ],
      "metadata": {
        "id": "Ou30crokXzel"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}